<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>http://localhost:4000</link>
    <description>
      A simple, beautiful theme for Jekyll that emphasizes content rather than aesthetic fluff.
    </description>
    
        
            <item>
                <title>Understanding Gradient Descent</title>
                <link>http://localhost:4000/jekyll/update/2017/04/15/gradient-descent/</link>
                <content:encoded>
                    <![CDATA[
                    <h3 id="the-problem-statement">The Problem statement</h3>
<p>Given a function <script type="math/tex">f : X \rightarrow R</script> where <script type="math/tex">X = (x_1, x_2, ... x_n)</script> is a n-dimensional vector, find a minima <script type="math/tex">X^*</script> equation (a point for which <script type="math/tex">f(X^*)</script> is a minimum)</p>

<h3 id="2-dimensional-case">2 dimensional case</h3>
<p>For the sake of simplicity and notation, we will first consider the case when the vector X is in 2-dimensional space i.e. <script type="math/tex">X = (x_1, x_2)</script>. The graph for this function might look something like this (this one happens to be a plot of <script type="math/tex">z = sin(x) + sin(y)</script>)</p>

<p>[Image]</p>

<p>Imagine now that this graph represents the some part of surface of the earth and you happen to be standing at some point in it. Your goal is head to the deepest point possible (in other words, head to the point with the lowest elevation).</p>

<p>The first thing you’re probably going to think of (without even being aware of it) is this: which direction do I take the next step? Of course you won’t take a step uphill from that point. Simple logic tells us to head downwards along the direction of steepest decline (assuming of course that you don’t slip and fall). This is the exact thinking behind the gradient descent algorithm: <strong>find the direction of maximum increasing (positive) gradient at the current point and take a small step in the opposite direction.</strong></p>

<p>Now, by the rules of calculus, a small change in the function <script type="math/tex">\Delta f</script> due to a small change in the vector <script type="math/tex">\Delta X = (\Delta x_1, \Delta x_2)</script> is given by:</p>

<script type="math/tex; mode=display">\Delta f = \frac{\partial f}{\partial x_1}.\Delta x_1 + \frac{\partial f}{\partial x_2}.\Delta x_2</script>

<p>or, in vector notation</p>

<script type="math/tex; mode=display">\Delta f = \nabla f . \Delta X,  
	where \nabla f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2})</script>

<p>Our goal now is to choose the delta vector <script type="math/tex">\Delta X = (\Delta x_1, \Delta x_2)</script> so as to make sure that:</p>
<ul>
  <li>The value <script type="math/tex">\Delta f</script> is negative (so that we know we are moving in the direction of decreasing gradient, or downwards)</li>
  <li>The magnitude of <script type="math/tex">\Delta f</script> is maximum along this direction</li>
</ul>

<p>If we choose <script type="math/tex">\Delta f</script> as follows: <script type="math/tex">\Delta X = -\eta.\nabla f</script>, then</p>

<script type="math/tex; mode=display">\Delta f = \mathbf{\nabla f}.(-\eta \mathbf{\nabla f}) = -\eta.(\mathbf{\nabla f.\nabla f}) = -\eta.(positive value) = negative</script>

<p><br />
<strong>The delta vector is now parallel to the gradient vector at that point</strong> which according to the Cauchy-Schwartz inequality maximizes the magnitude of the dot product of the 2 vectors (hence maximizing <script type="math/tex">\left \| \Delta f \right \|</script> ). Note that the positive constant <script type="math/tex">\eta</script>  does not change the direction of the vector, only scales its value. Since we are only looking to find the direction of maximization of descent, scaling the vector does not affect our goal.</p>

<p><script type="math/tex">\eta</script> is also called the <strong>learning rate</strong>, a value that determines how large or small a step we take while descending. <script type="math/tex">\eta</script> should be small so as not to introduce error (if the step is too large, we might miss the minima). Also it should not be too small that the descent takes a long time.</p>

<p>And so we arrive at the update function for the variable vector X:</p>

<script type="math/tex; mode=display">\mathbf{X_{new}} = \mathbf{X_{old}} - \eta.\mathbf{\nabla f}</script>

<p>Although we have described the problem for the special case of 2 dimensions, it easily extends to the more general case of N dimensions. The formula above remains the same.
<br /></p>
<h3 id="iterations-and-when-to-stop-todo">Iterations and when to stop (TODO)</h3>

<p><br /></p>
<h3 id="drawbacks">Drawbacks</h3>
<p>Note that, in general, a function can have multiple local minima and a global minima. Based on how the gradient descent algorithm is initialized (the initial value that we choose for the variable vector) and the structure of the function itself, we may end up at any one of these local minima. <strong>So gradient descent does not always result in the global minima</strong>.</p>

<p><br /></p>
<h3 id="proof-todo">Proof (TODO)</h3>
<p>According to the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwartz inequality</a> for any 2 vectors <script type="math/tex">\mathbf{u}</script> and<script type="math/tex">\mathbf{v}</script> of the same size, <script type="math/tex">\| \mathbf{u}.\mathbf{v} \| \le \| \mathbf{u} \| . \| \mathbf{v} \|</script> and the equality happens when u and v are equal.</p>

<p>If we restrict our change vector <script type="math/tex">\Delta X</script> (or the incremental step) to be small, as restricted by <script type="math/tex">\| \Delta X \| \le \epsilon</script>for some small <script type="math/tex">\epsilon > 0</script> then according to the Cauchy-Schwartz inequality, the magnitude of change is</p>

<script type="math/tex; mode=display">| \Delta f | = | \mathbf{\nabla f }. \mathbf{\Delta X} |  \le | \mathbf{\nabla f} |.| \mathbf{\Delta X}| \le |\mathbf{\nabla f}|.\epsilon</script>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/jekyll/update/2017/04/15/gradient-descent/</guid>
                <description>
                    
                </description>
                <pubDate>Sat, 15 Apr 2017 20:20:57 -0400</pubDate>
                <author>Abhinav Choudhury</author>
            </item>
        
    
        
            <item>
                <title>Fish classification using Keras</title>
                <link>http://localhost:4000/jekyll/update/2017/04/15/fish-classification-keras/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Keras is a powerful framework for building deep learning models that facilitates rapid prototyping. It works on top of a tensor manipulation engine (either Tensorflow or Theano) which are optimized to leverage the GPU for fast, parallel matrix computations.</p>

<h3 id="data-organization-and-preprocessing">Data organization and preprocessing</h3>
<p>For an image classification task, you are likely to find your image dataset in a number of different formats:</p>
<ol>
  <li>A folder containing all the training images and a separate text file designating the class ids for each image</li>
  <li>Images named by concatenating their respective classes with an unique identifier eg. dog_001.jpg, cat_002.jpg, etc</li>
  <li>Separate directories for each class containing images of that class</li>
</ol>

<p>The image preprocessing library in Keras expects data to be in the third format. These inbuilt image reading and batching functionalities in Keras makes life a lot easier by allowing you to</p>

<h3 id="creating-training-and-validation-sets">Creating training and validation sets</h3>
<p>We need to split the data into a training set which will be used to train our deep learning model, and a validation set which would allow us to test how well our model classifies instances it has not seen before. This is a measure of the <strong>generalization capability</strong> of the model, and is crucial in reducing overfitting. The ratio of the split is usually set at 70:30 or 80:20.</p>

<p>The idea is to use a statistical sampling technique called the <strong>Stratified Split</strong>. If the dataset is split randomly and there are only a few of images for certain classes, it is likely that either the training set or the validation set will end up with no instances of those classes. When the model observes no instances of a certain class in its training data, its classification performance on instances of that class will likely be poor. Stratified split solves this problem by sampling instances from each class based on the set split ratio, thus ensuring both the training and validation sets have instances of every class.</p>

<p>Below is a function that uses Scikit-learn’s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html">StratifiedShuffleSplit</a> function to create a 80:20 split</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedShuffleSplit</span>

<span class="k">def</span> <span class="nf">stratifiedSplit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="n">sss</span> <span class="o">=</span> <span class="n">StratifiedShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2017</span><span class="p">)</span>
        <span class="n">sss</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">sss</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
                <span class="k">break</span>
        <span class="k">return</span><span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span>
</code></pre>
</div>

<p>Next, we use the function to create training and validation datasets:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>
<span class="n">ORIGINAL_DATAPATH</span> <span class="o">=</span> <span class="s">"./train/"</span>
<span class="n">TRAIN_PATH</span> <span class="o">=</span> <span class="s">"./TRAIN/"</span>
<span class="n">VALIDATION_PATH</span> <span class="o">=</span> <span class="s">"./VALID/"</span>

<span class="k">def</span> <span class="nf">createTrainAndValidationDatasets</span><span class="p">(</span><span class="n">datapath</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"### Sampling training and validation datasets..."</span><span class="p">)</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s">"ALB"</span><span class="p">,</span> <span class="s">"BET"</span><span class="p">,</span> <span class="s">"DOL"</span><span class="p">,</span> <span class="s">"LAG"</span><span class="p">,</span> <span class="s">"NoF"</span><span class="p">,</span> <span class="s">"OTHER"</span><span class="p">,</span> <span class="s">"SHARK"</span><span class="p">,</span> <span class="s">"YFT"</span><span class="p">]</span>

        <span class="k">def</span> <span class="nf">makeDirectoryStructure</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
                        <span class="k">for</span> <span class="n">cl</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
                                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span> <span class="o">+</span> <span class="n">cl</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                        <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">cl</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
                                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span> <span class="o">+</span> <span class="n">cl</span><span class="p">)</span>

        <span class="n">img_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">img_classes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">cl</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
                <span class="n">class_dir</span> <span class="o">=</span> <span class="n">ORIGINAL_DATAPATH</span> <span class="o">+</span> <span class="n">cl</span> <span class="o">+</span> <span class="s">"/"</span>
                <span class="n">filepaths</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">class_dir</span> <span class="o">+</span> <span class="s">"*.jpg"</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">filepath</span> <span class="ow">in</span> <span class="n">filepaths</span><span class="p">:</span>
                        <span class="n">img_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">))</span>
                        <span class="n">img_classes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cl</span><span class="p">)</span>

        <span class="n">train_idx</span><span class="p">,</span> <span class="n">valid_idx</span> <span class="o">=</span> <span class="n">stratifiedSplit</span><span class="p">(</span><span class="n">img_names</span><span class="p">,</span> <span class="n">img_classes</span><span class="p">)</span>
        <span class="n">makeDirectoryStructure</span><span class="p">(</span><span class="n">TRAIN_PATH</span><span class="p">)</span>
        <span class="n">makeDirectoryStructure</span><span class="p">(</span><span class="n">VALIDATION_PATH</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">train_idx</span><span class="p">:</span>
                <span class="n">img_name</span> <span class="o">=</span> <span class="n">img_names</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">img_class</span> <span class="o">=</span> <span class="n">img_classes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">shutil</span><span class="o">.</span><span class="n">copyfile</span><span class="p">(</span><span class="n">ORIGINAL_DATAPATH</span> <span class="o">+</span> <span class="n">img_class</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">img_name</span><span class="p">,</span>\
                	<span class="n">TRAIN_PATH</span> <span class="o">+</span> <span class="n">img_class</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">img_name</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">valid_idx</span><span class="p">:</span>
                <span class="n">img_name</span> <span class="o">=</span> <span class="n">img_names</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">img_class</span> <span class="o">=</span> <span class="n">img_classes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">shutil</span><span class="o">.</span><span class="n">copyfile</span><span class="p">(</span><span class="n">ORIGINAL_DATAPATH</span> <span class="o">+</span> <span class="n">img_class</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">img_name</span><span class="p">,</span>
               		<span class="n">VALIDATION_PATH</span> <span class="o">+</span> <span class="n">img_class</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">img_name</span><span class="p">)</span>

<span class="n">createTrainAndValidationDatasets</span><span class="p">(</span><span class="n">ORIGINAL_DATAPATH</span><span class="p">)</span>
</code></pre>
</div>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/jekyll/update/2017/04/15/fish-classification-keras/</guid>
                <description>
                    
                </description>
                <pubDate>Sat, 15 Apr 2017 20:20:57 -0400</pubDate>
                <author>Abhinav Choudhury</author>
            </item>
        
    
  </channel>
</rss>
